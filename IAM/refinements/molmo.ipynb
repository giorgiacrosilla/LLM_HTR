{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import difflib\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    'allenai/Molmo-7B-D-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/Molmo-7B-D-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Specify the input and output folders\n",
    "input_folder = 'IAMa_cropped'  # Folder containing input images\n",
    "output_folder = 'transcriptions_IAM2_molmo'  # Folder to save final transcriptions\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define system prompt and user prompt\n",
    "USER_PROMPT = \"Please transcribe as accurately as possible the handwritten portions of the provided image.\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant specialized in transcribing handwritten text from images. Please follow these guidelines:\n",
    "1. Examine the image carefully and identify all handwritten text.\n",
    "2. Transcribe ONLY the handwritten text. Ignore any printed or machine-generated text in the image.\n",
    "3. Maintain the original structure of the handwritten text, including line breaks and paragraphs.\n",
    "4. Do not attempt to correct spelling or grammar in the handwritten text. Transcribe it exactly as written.\n",
    "5. Do not describe the image or its contents.\n",
    "6. Do not introduce or contextualize the transcription.\n",
    "Please begin your response directly with the transcribed text. Remember, your goal is to provide an accurate transcription of ONLY the handwritten portions of the text, preserving its original form as much as possible.\"\"\"\n",
    "\n",
    "# Define refinement prompts\n",
    "refinement_prompts = [\n",
    "    \"Review the original image and your previous transcription. Focus on correcting any spelling errors, punctuation mistakes, or missed words. Ensure the transcription accurately reflects the handwritten text.\",\n",
    "    \"Examine the structure of the transcription. Are paragraphs and line breaks correctly represented? Adjust the layout to match the original handwritten text more closely.\",\n",
    "    \"Make a final pass over the transcription, comparing it closely with the original image. Make any last corrections or improvements to ensure the highest possible accuracy. Do not add any introduction or contextualization you might have added to the transcribed text. Start directly with the transcription.\"\n",
    "]\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Ensure image is in RGB format and correct shape.\n",
    "    \"\"\"\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    img_array = np.array(image)\n",
    "    if img_array.ndim == 2:  # Grayscale image\n",
    "        img_array = np.stack((img_array,) * 3, axis=-1)\n",
    "    elif img_array.shape[2] == 1:  # Single channel\n",
    "        img_array = np.repeat(img_array, 3, axis=-1)\n",
    "    return Image.fromarray(img_array)\n",
    "\n",
    "def print_differences(original, modified):\n",
    "    \"\"\"\n",
    "    Print differences between the original and modified transcriptions.\n",
    "    \"\"\"\n",
    "    diff = difflib.ndiff(original.splitlines(), modified.splitlines())\n",
    "    return '\\n'.join(diff)\n",
    "\n",
    "# Process each image in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Create a folder for the current image's transcriptions\n",
    "        image_output_folder = os.path.join(output_folder, os.path.splitext(filename)[0])\n",
    "        os.makedirs(image_output_folder, exist_ok=True)\n",
    "\n",
    "        # Open and preprocess the image\n",
    "        image = Image.open(image_path)\n",
    "        image = preprocess_image(image)\n",
    "\n",
    "        # Combine system prompt and user prompt\n",
    "        full_prompt = f\"{SYSTEM_PROMPT}\\n\\n{USER_PROMPT}\"\n",
    "\n",
    "        # Process the image and text\n",
    "        inputs = processor.process(\n",
    "            images=[image],\n",
    "            text=full_prompt\n",
    "        )\n",
    "\n",
    "        # Move inputs to the model's device and add a batch dimension\n",
    "        inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate the initial transcription\n",
    "        output = model.generate_from_batch(\n",
    "            inputs,\n",
    "            GenerationConfig(max_new_tokens=200, stop_strings=\"<|endoftext|>\"),\n",
    "            tokenizer=processor.tokenizer\n",
    "        )\n",
    "\n",
    "        # Decode the generated tokens to text\n",
    "        generated_tokens = output[0, inputs['input_ids'].size(1):]\n",
    "        generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Save the initial transcription in the folder for this image\n",
    "        initial_output_path = os.path.join(image_output_folder, 'refinement_step_0.txt')\n",
    "        with open(initial_output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(generated_text)\n",
    "\n",
    "        print(f\"Initial transcription for {filename}:\\n{generated_text}\\n\")\n",
    "\n",
    "        # Perform refinement steps\n",
    "        refined_text = generated_text\n",
    "        previous_refined_text = refined_text\n",
    "\n",
    "        for step, refinement_prompt in enumerate(refinement_prompts, start=1):\n",
    "            # Create refinement prompt\n",
    "            refinement_full_prompt = f\"{SYSTEM_PROMPT}\\n\\n{refinement_prompt}\\n\\nTranscription:\\n{refined_text}\\n\\nOriginal Image: {filename}\"\n",
    "\n",
    "            # Process the image and text for refinement\n",
    "            refinement_inputs = processor.process(\n",
    "                images=[image],\n",
    "                text=refinement_full_prompt\n",
    "            )\n",
    "\n",
    "            # Move refinement inputs to model's device\n",
    "            refinement_inputs = {k: v.to(model.device).unsqueeze(0) for k, v in refinement_inputs.items()}\n",
    "\n",
    "            # Generate the refined transcription\n",
    "            refined_output = model.generate_from_batch(\n",
    "                refinement_inputs,\n",
    "                GenerationConfig(max_new_tokens=200, stop_strings=\"<|endoftext|>\"),\n",
    "                tokenizer=processor.tokenizer\n",
    "            )\n",
    "\n",
    "            # Decode refined tokens into text\n",
    "            refined_generated_tokens = refined_output[0, refinement_inputs['input_ids'].size(1):]\n",
    "            refined_text = processor.tokenizer.decode(refined_generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            # Save the refined transcription in the folder for this image\n",
    "            refinement_output_path = os.path.join(image_output_folder, f'refinement_step_{step}.txt')\n",
    "            with open(refinement_output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(refined_text)\n",
    "\n",
    "            # Display the refined transcription and differences\n",
    "            differences = print_differences(previous_refined_text, refined_text)\n",
    "            print(f\"Refined transcription for {filename} (Step {step}):\\n{refined_text}\\n\")\n",
    "            print(f\"Differences from previous transcription (Step {step - 1}):\\n{differences}\\n\")\n",
    "\n",
    "            # Update for the next iteration\n",
    "            previous_refined_text = refined_text\n",
    "\n",
    "        print(f\"All refinements for {filename} saved in {image_output_folder}\")\n",
    "\n",
    "print(\"All images processed with refinements.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
