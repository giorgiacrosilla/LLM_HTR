{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    'allenai/Molmo-7B-O-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'allenai/Molmo-7B-O-0924',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "\n",
    "input_folder = 'aachen_validation_set'  \n",
    "output_folder = 'transcriptions_IAM_molmo'  \n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define system prompt and user prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant specialized in transcribing handwritten text from images. Please follow these guidelines:\n",
    "1. Examine the image carefully and identify all handwritten text.\n",
    "2. Transcribe ONLY the handwritten text.\n",
    "3. Maintain the original structure of the handwritten text, including line breaks and paragraphs.\n",
    "4. Do not attempt to correct spelling or grammar in the handwritten text. Transcribe it exactly as written.\n",
    "5. Do not describe the image or its contents.\n",
    "6. Do not introduce or contextualize the transcription.\n",
    "Please begin your response directly with the transcribed text. Remember, your goal is to provide an accurate transcription of ONLY the handwritten portions of the text, preserving its original form as much as possible.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Please transcribe the handwritten text in this image as accurately as possible, respecting line breaks. Do not describe the fields of the image(\"body text\" or \"signature\"), stick only to the text as it is.\"\"\"\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert image to RGB if it's not already\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(image)\n",
    "\n",
    "    # Ensure the image is in the correct shape (height, width, channels)\n",
    "    if img_array.ndim == 2:  # Grayscale image\n",
    "        img_array = np.stack((img_array,) * 3, axis=-1)\n",
    "    elif img_array.shape[2] == 1:  # Single channel\n",
    "        img_array = np.repeat(img_array, 3, axis=-1)\n",
    "\n",
    "    # Return the image as a PIL Image\n",
    "    return Image.fromarray(img_array)\n",
    "\n",
    "# Process each image in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "        # Construct full file paths\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_transcription.txt\")\n",
    "        \n",
    "        # Open the image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Preprocess the image\n",
    "        image = preprocess_image(image)\n",
    "\n",
    "        # Combine system prompt and user prompt\n",
    "        full_prompt = f\"{SYSTEM_PROMPT}\\n\\n{USER_PROMPT}\"\n",
    "\n",
    "        # Process the image and text\n",
    "        inputs = processor.process(\n",
    "            images=[image],\n",
    "            text=full_prompt\n",
    "        )\n",
    "\n",
    "        # Move inputs to the correct device and add batch dimension\n",
    "        inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}  # Unsqueeze to add batch dimension\n",
    "\n",
    "        # Generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated\n",
    "        output = model.generate_from_batch(\n",
    "            inputs,\n",
    "            GenerationConfig(max_new_tokens=200, stop_strings=\"<|endoftext|>\"),\n",
    "            tokenizer=processor.tokenizer\n",
    "        )\n",
    "\n",
    "        # Only get generated tokens; decode them to text\n",
    "        generated_tokens = output[0, inputs['input_ids'].size(1):]\n",
    "        generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Save the transcription to a file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(generated_text)\n",
    "        \n",
    "        print(f\"Transcription for {filename} saved to {output_path}\")\n",
    "\n",
    "print(\"All images processed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
